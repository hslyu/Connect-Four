import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision.transforms as T

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class DQN(nn.Module):
    def __init__(self, h, w, outputs):
        super(DQN, self).__init__()

        linear_input_size = h*w
        self.fc1 = nn.Linear(linear_input_size, linear_input_size, bias=True)
        self.fc2 = nn.Linear(linear_input_size, linear_input_size, bias=True)
        self.fc3 = nn.Linear(linear_input_size, linear_input_size*2, bias=True)
        self.fc4 = nn.Linear(linear_input_size*2, linear_input_size*2, bias=True)
        self.fc5 = nn.Linear(linear_input_size*2, linear_input_size*2, bias=True)
        self.fc6 = nn.Linear(linear_input_size*2, linear_input_size*4, bias=True)
        self.fc7 = nn.Linear(linear_input_size*4, linear_input_size*4, bias=True)
        self.fc8 = nn.Linear(linear_input_size*4, linear_input_size*4, bias=True)
        self.fc9 = nn.Linear(linear_input_size*4, linear_input_size*8, bias=True)
        self.fc10 = nn.Linear(linear_input_size*8, linear_input_size*8, bias=True)
        self.fc11 = nn.Linear(linear_input_size*8, linear_input_size*8, bias=True)
        self.fc12 = nn.Linear(linear_input_size*8, linear_input_size*4, bias=True)
        self.fc13 = nn.Linear(linear_input_size*4, linear_input_size*4, bias=True)
        self.fc14 = nn.Linear(linear_input_size*4, linear_input_size*4, bias=True)
        self.fc15 = nn.Linear(linear_input_size*4, linear_input_size*2, bias=True)
        self.fc16 = nn.Linear(linear_input_size*2, outputs)

    def forward(self, x):
        x = x.to(device).float()
        x = x.view(x.size(0),-1)
        residual = x
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x)+residual)
        x = F.relu(self.fc3(x))
        residual = x
        x = F.relu(self.fc4(x))
        x = F.relu(self.fc5(x)+residual)
        x = F.relu(self.fc6(x))
        residual = x
        x = F.relu(self.fc7(x))
        x = F.relu(self.fc8(x)+residual)
        x = F.relu(self.fc9(x))
        residual = x
        x = F.relu(self.fc10(x))
        x = F.relu(self.fc11(x)+residual)
        x = F.relu(self.fc12(x))
        residual = x
        x = F.relu(self.fc13(x))
        x = F.relu(self.fc14(x)+residual)
        x = F.relu(self.fc15(x))
        return self.fc16(x)

if __name__ == "__main__":
    import numpy as np
    from board import *
    import warnings
    warnings.filterwarnings("ignore", category=DeprecationWarning)
    device = torch.device("cpu")

    n = DQN(3,5,5)
    board = np.random.randint(2, size=(3,1,3,5))
#    am = torch.cat([available_moves(b.reshape((3,5))).tolist() for b in board])
    am = [available_moves(b.reshape(3,5),True) for b in board]
    print(board)
    print(f'{am = }')
    t=n(torch.from_numpy(board))[0]
    print('-----')
    print('-----')
    print(f'{n(torch.from_numpy(board)) = }')
    b = torch.tensor([t[am[i]].sort(descending=True).values[0] for i,t in enumerate(n(torch.from_numpy(board)))])
    print('-----')
    print('-----')
    print(b)
    print('-----')
    print('-----')
    for i,t in enumerate(n(torch.from_numpy(board))):
        print(t[am[i]].sort()[-1])
        print('-----')

#    print(f'{n(torch.from_numpy(board)) = }\n')
#    print(f'{n(torch.from_numpy(board))[[True, True, False, False, False]] = }\n')
#    print(f'{n(torch.from_numpy(board)).sort(descending=True).indices.tolist() = }\n')
#    print(f'{n(torch.from_numpy(board)) = }\n')
#    print(f'{n(torch.from_numpy(board)).max(1) = }\n')
#    print(f'{n(torch.from_numpy(board)).max(1)[1] = }\n')
#    print(f'{n(torch.from_numpy(board)).max(1)[1].item() =}\n')
